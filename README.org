* Istio Hands-On for Kubernetes

My notes from the course on Udemy.
* What is Istio?
- [[https://thenewstack.io/how-does-service-discovery-work-in-kubernetes/][Service discovery mechanism]] in kubernetes allows network calls among containers
- Service Mesh replaces the kubernetes service communication mechanism
  - All network calls go through the service mesh layer
  - The service mesh can apply *mesh logic* before the request is routed to the target container or it could run after, or both
- What sorts of stuff happens in the service mesh layer?
  - Telemetry
  - Potentially rerouting requests based on certain requirements (sounds familiar)
- Istio injects a container to each pod
  - This container is called a proxy.  The ones I've seen are named =istio-proxy=
  - In istio, the proxies communicate with the mesh layer and the app container.  App containers don't talk directly to the mesh layer or each other.
- Istio's got all sorts of obtuse nautical names that make it harder to understand when you're a newb
- They simplified some of the nautical names to a single =istiod= daemon (it used to be called "pilot" ðŸ™„) that lives in the =istio-system= pod in your cluster
  - This is referred to as the "control plane"
  - Kiali is a user interface pod in the control plane. Presumably its name will make sense at some point.
- What changes do we need to make to our application containers to make this work?
  - None! (in theory)
  - There are some changes you need to make for tracing
  - TODO determine whether we are using this sort of tracing
* Hands on Istio demo
- Repo for the course https://github.com/DickChesterwood/istio-fleetman
** Getting istio running
(in the warmup-exercise directory)
*** Apply the manifests to install istio
    #+begin_src sh
      kubectl apply -f 1-istio-init.yaml
    #+end_src

Sets up a namespace and initializes a lot of stuff
*** To remove
    #+begin_src sh
      kubectl delete -f 1-istio-init.yaml
    #+end_src
*** Apply the manifests to start istio
    #+begin_src sh
      kubectl apply -f 2-istio-minikube.yaml
    #+end_src

Creates a bunch of deployments in the istio namespace
*** Install the Kiali secrets file
    #+begin_src sh
      kubectl apply -f 3-kiali-secret.yaml
    #+end_src

The username and password are base64 encoded.  You can decode them with

#+begin_src sh
  echo "YWRtaW4=" | base64 -d
#+end_src
*** Injecting the istio-proxy sidecar
The istio-proxy is injected into each pod in a namespace via the istiod based on the presence of a label in the namespace.

#+begin_src sh
  kubectl label namespace default istio-injection=enabled
#+end_src
To remove:
#+begin_src sh
  kubectl label namespace default istio-injection-
#+end_src

Whew this syntax is garbage.  Sort of like deleting a branch in git used to require colons in a special place.  Much easier to use the manifest :)

Istio uses init containers to do the injecting https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ .  That explains the =Completed= =istio-init= we see when inspecting the containers within a pod.

*** Deploying up the application

#+begin_src sh
kubectl apply -f 4-application-full-stack.yaml
#+end_src

In order to visit the Fleet Manager web app, I am able to set up port forwarding for the webapp directly but I don't think that's going through istio.

*** How do I expose my webapp through istio so I can access it from localhost running in Kubernetes for Docker Desktop?
- I tried following https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/#determining-the-ingress-ip-and-ports
  - I determined that my ingress IP is 127.0.0.1 because I'm using Docker Desktop.
  - I determined that I needed to get the port from a =NodePort= because the =LoadBalancer='s external IP was stuck in a =<pending>= state.
  - I was not able to connect to the ingress successfully

- I'm going to take a step back and follow istios https://istio.io/latest/docs/setup/getting-started/ guide to determine whether the problem exists in the Udemy course's istio setup.
  - Still not successful.  The demo istio deployment has the same issue
  - Checked for any services running on conflicting ports, found one on port 80 and removed it
  - Was able to port forward the exposed nodeport from the pod running the ingressgateway, I get an empty response, but I can't talk to the application
  - Resetting kubernetes cluster seems to have done the trick! https://github.com/docker/for-mac/issues/4903
  - After following the steps in the getting started guide I got:

#+begin_src sh
  curl -I localhost/productpage
  HTTP/1.1 200 OK
  content-type: text/html; charset=utf-8
  content-length: 5183
  server: istio-envoy
  date: Fri, 28 Jan 2022 18:45:50 GMT
  x-envoy-upstream-service-time: 134
#+end_src

- When I revisit the example from the Udemy course

#+begin_src sh
  curl -I http://localhost:30080/
  HTTP/1.1 200 OK
  server: istio-envoy
  date: Fri, 28 Jan 2022 18:58:58 GMT
  content-type: text/html
  content-length: 816
  last-modified: Fri, 11 Oct 2019 19:15:44 GMT
  etag: "5da0d4e0-330"
  accept-ranges: bytes
  x-envoy-upstream-service-time: 0
  x-envoy-decorator-operation: fleetman-webapp.default.svc.cluster.local:80/*
#+end_src

which means it's working!


** Finding perf problems
- We can use Kiali at localhost:31000 to diagnose issues with the services in the service mesh
- We can use Jaeger to view request traces at localhost:31001
- Interesting to think about service degredation or removal in the context of our prod microservices
- We can modify the service mesh to drop a request that is causing knock on errors in our system
- In this case we want to use a timeout, see https://istio.io/latest/docs/tasks/traffic-management/request-timeouts/

*** fleetman-driver-monitoring

#+begin_src yaml
  kind: VirtualService
  metadata:
    name: fleetman-driver-monitoring
  ...
  http:
  ...
    timeout: 1s
#+end_src


- Reapply the app's manifest
- Observe that the vehicle positions update faster now in the app rather than taking on the order of minutes to fully update.

* Introducing Envoy
- Envoy's an open source project distinct from istio that comes from Lyft originally.
- There's confusing terminology for the proxy that istio injects into all pods in kubernetes.  They're called =envoy=, =proxy=, =sidecar=.
- I think the course typically calls it the proxy.
- Sidecar is just a method of injecting pods in Kubernetes
- Envoy is probably most specific because it is an envoy proxy
- Envoy has distinct concepts from istio, so it's more convenient to use istio in the course's opinion
  - For example envoy uses =clusters= to mean something distinct from kubernetes clusters
- The proxies are collectively called the data plane in istio

* Telemetry
- In =_course_files/1\ Telemetry=
** Kiali
- Visit [[localhost:30080][localhost:30080]] to see it run
- Distinction between the sidecar and traffic routing stuff (like VirtualServices and Gateways)
  - We just need the sidecar envoy proxies for getting telemetry
- I wonder how [[https://github.com/kiali/kiali][Kiali]] renders its graphs.
  -Looks like it's using https://cytoscape.org/
- You can see response times in the workload graph
- You can turn on Traffic Animation to see the dynamic traffic flowing through the system
  - The author mentions this as a killer feature of Istio
  - It's made possible through the envoy proxies
** Kiali dynamic traffic routing
- Kiali can suspend traffic to a service directly from the UI
** Distributed Tracing
You can visit Jaeger at localhost:31001/jaeger

- Istio can support distributed tracing without having to change the application code
- It works via the Envoy proxy running in the sidecar
- Traces and spans use similar terminology to Datadog
- Sometimes you may see a doubling up of spans due to the proxies handling requests
  - I guess if you're not using the proxies you may not see as many spans
- The hardest thing about working with distributed traces is finding your traces
  - I definitely feel this!
** Why you need to propagate headers
- Because that's how istio instruments distributed tracing via the sidecar proxies

* Traffic Management
